{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, BartModel\n",
    "\n",
    "#from optimizer import AdamW # (amin)\n",
    "from transformers import AdamW # (amin)\n",
    "\n",
    "\n",
    "TQDM_DISABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartWithClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=7):\n",
    "        super(BartWithClassifier, self).__init__()\n",
    "\n",
    "        self.bart = BartModel.from_pretrained(\"facebook/bart-large\", local_files_only=True) # (amin) changed local_files_only=True -> local_files_only=False\n",
    "        self.classifier = nn.Linear(self.bart.config.hidden_size, num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Use the BartModel to obtain the last hidden state\n",
    "        outputs = self.bart(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        cls_output = last_hidden_state[:, 0, :]\n",
    "\n",
    "        # Add an additional fully connected layer to obtain the logits\n",
    "        logits = self.classifier(cls_output)\n",
    "\n",
    "        # Return the probabilities\n",
    "        probabilities = self.sigmoid(logits)\n",
    "        return probabilities\n",
    "    \n",
    "def get_args():\n",
    "    args = argparse.Namespace(seed=11711, use_gpu=False)\n",
    "    return args\n",
    "\n",
    "def seed_everything(seed=11711):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, test_data, device):\n",
    "    \"\"\"\n",
    "    This function measures the accuracy of our model's prediction on a given train/validation set\n",
    "    We measure how many of the seven paraphrase types the model has predicted correctly for each data point.\n",
    "    So, if the models prediction is [1,1,0,0,1,1,0] and the true label is [0,0,0,0,1,1,0], this predicition\n",
    "    has an accuracy of 5/7, i.e. 71.4% .\n",
    "    \"\"\"\n",
    "    all_pred = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data): # (amin) added tqdm\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predicted_labels = (outputs > 0.5).int()\n",
    "\n",
    "            all_pred.append(predicted_labels)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_predictions = torch.cat(all_pred, dim=0)\n",
    "    all_true_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    true_labels_np = all_true_labels.cpu().numpy()\n",
    "    predicted_labels_np = all_predictions.cpu().numpy()\n",
    "\n",
    "    # Compute the accuracy for each label\n",
    "    accuracies = []\n",
    "    for label_idx in range(true_labels_np.shape[1]):\n",
    "        correct_predictions = np.sum(\n",
    "            true_labels_np[:, label_idx] == predicted_labels_np[:, label_idx]\n",
    "        )\n",
    "        total_predictions = true_labels_np.shape[0]\n",
    "        label_accuracy = correct_predictions / total_predictions\n",
    "        accuracies.append(label_accuracy)\n",
    "\n",
    "    # Calculate the average accuracy over all labels\n",
    "    accuracy = np.mean(accuracies)\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def finetune_paraphrase_detection(args):\n",
    "    model = BartWithClassifier()\n",
    "    device = torch.device(\"cuda\") if args.use_gpu else torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    train_dataset = pd.read_csv(\"data/etpc-paraphrase-train.csv\", sep=\"\\t\")\n",
    "    test_dataset = pd.read_csv(\"data/etpc-paraphrase-detection-test-student.csv\", sep=\"\\t\")\n",
    "\n",
    "    # TODO You might do a split of the train data into train/validation set here\n",
    "    # (or in the csv files directly)\n",
    "    train_data = transform_data(train_dataset)\n",
    "    test_data = transform_data(test_dataset)\n",
    "\n",
    "    print(f\"Loaded {len(train_dataset)} training samples.\")\n",
    "\n",
    "    model = train_model(model, train_data, dev_data, device)\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    accuracy = evaluate_model(model, dev_data, device)\n",
    "    print(f\"The accuracy of the model is: {accuracy:.3f}\")\n",
    "\n",
    "    test_ids = test_dataset[\"id\"]\n",
    "    test_results = test_model(model, test_data, test_ids, device)\n",
    "    test_results.to_csv(\n",
    "        \"predictions/bart/etpc-paraphrase-detection-test-output.csv\", index=False, sep=\"\\t\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = get_args()\n",
    "seed_everything(args.seed)\n",
    "#finetune_paraphrase_detection(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BartWithClassifier()\n",
    "device = torch.device(\"cuda\") if args.use_gpu else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_dataset = pd.read_csv(\"../data/etpc-paraphrase-train.csv\", sep=\"\\t\", converters={'paraphrase_types': pd.eval, 'sentence1_tokenized': pd.eval, 'sentence2_tokenized': pd.eval})\n",
    "test_dataset = pd.read_csv(\"../data/etpc-paraphrase-detection-test-student.csv\", sep=\"\\t\", converters={'sentence1_tokenized': pd.eval, 'sentence2_tokenized': pd.eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (amin)\n",
    "\n",
    "print(type(train_dataset['paraphrase_types'][0]), train_dataset['paraphrase_types'][0]) \n",
    "print(type(train_dataset['sentence1_tokenized'][0]), train_dataset['sentence1_tokenized'][0]) \n",
    "print('Shape of train dataframe:', train_dataset.shape)\n",
    "print('Shape of test dataframe:', test_dataset.shape)\n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(dataset, max_length=512, batch_size=2, tokenizer_name=\"facebook/bart-large\"): # (am9n) add batch_size=2, tokenizer_name=\"facebook/bart-large\"\n",
    "    \"\"\"\n",
    "    dataset: pd.DataFrame\n",
    "\n",
    "    Turn the data to the format you want to use.\n",
    "\n",
    "    1. Extract the sentences from the dataset. We recommend using the already split\n",
    "    sentences in the dataset.\n",
    "    2. Use the AutoTokenizer from_pretrained to tokenize the sentences and obtain the\n",
    "    input_ids and attention_mask.\n",
    "    3. Currently, the labels are in the form of [2, 5, 6, 0, 0, 0, 0]. This means that\n",
    "    the sentence pair is of type 2, 5, and 6. Turn this into a binary form, where the\n",
    "    label becomes [0, 1, 0, 0, 1, 1, 0]. Be careful that the test-student.csv does not\n",
    "    have the paraphrase_types column. You should return a DataLoader without the labels.\n",
    "    4. Use the input_ids, attention_mask, and binary labels to create a TensorDataset.\n",
    "    Return a DataLoader with the TensorDataset. You can choose a batch size of your\n",
    "    choice.\n",
    "    \"\"\"\n",
    "    # raise NotImplementedError # (amin)\n",
    "\n",
    "    # (amin) [\n",
    "\n",
    "    # Extract sentences (problem implementing the suggestion)\n",
    "    sentences1 = dataset['sentence1'].tolist()\n",
    "    sentences2 = dataset['sentence2'].tolist()\n",
    "\n",
    "    #sentences1 = dataset['sentence1_tokenized'].tolist()\n",
    "    #sentences2 = dataset['sentence2_tokenized'].tolist()\n",
    "    \n",
    "    #print(sentences1)\n",
    "\n",
    "    # Tokenize sentences\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    \n",
    "    encoded = tokenizer(\n",
    "        sentences1,\n",
    "        sentences2,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    \n",
    "    # Convert labels to binary form, if available\n",
    "    if 'paraphrase_types' in dataset.columns: # since the test data does not have this column\n",
    "        binary_labels = []\n",
    "        for labels in dataset['paraphrase_types']:\n",
    "            # print(type(labels), labels)\n",
    "            binary_label = [0] * 7  # 7 classes\n",
    "            for label in labels:\n",
    "                if label != 0:  # Skip the padding zeros\n",
    "                    binary_label[label - 1] = 1 # labels range is 1-7\n",
    "            binary_labels.append(binary_label)\n",
    "        \n",
    "        binary_labels = torch.tensor(binary_labels)\n",
    "    \n",
    "        # Create TensorDataset with labels\n",
    "        tensor_dataset = TensorDataset(input_ids, attention_mask, binary_labels)\n",
    "    else:\n",
    "        # Create TensorDataset without labels\n",
    "        tensor_dataset = TensorDataset(input_ids, attention_mask)\n",
    "    \n",
    "    # Collate function to return dict (datasets.Dataset like) output instead of list (not compatible with the evaluation() function)\n",
    "    def collate_fn(batch):\n",
    "        input_ids = torch.stack([item[0] for item in batch])\n",
    "        attention_mask = torch.stack([item[1] for item in batch])\n",
    "        \n",
    "        # Check if binary_labels are present in the batch\n",
    "        if len(batch[0]) > 2:\n",
    "            labels = torch.stack([item[2] for item in batch])\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "\n",
    "    # Create DataLoader\n",
    "    #data_loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn) # uncomment if you want a dictionary\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=batch_size) # , shuffle=True (nover put this, because of test data)\n",
    "    \n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# TODO You might do a split of the train data into train/validation set here\n",
    "# (or in the csv files directly)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "train_df = train_dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the DataFrame into training and validation datasets\n",
    "train_frac = 0.8\n",
    "train_size = int(len(train_df) * train_frac)\n",
    "\n",
    "train_data = train_df.iloc[:train_size]\n",
    "dev_data = train_df.iloc[train_size:]\n",
    "\n",
    "train_data = transform_data(train_data) # (amin) change train_dataset -> train_df\n",
    "dev_data = transform_data(dev_data)\n",
    "test_data = transform_data(test_dataset)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} training samples.\")\n",
    "\n",
    "# Test if dataloader works as expected\n",
    "for data in train_data:\n",
    "    input_ids, attention_mask, binary_labels = data\n",
    "    print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
    "    print(f\"Binary Labels: {binary_labels.shape}\")\n",
    "\n",
    "    #print(f\"Input IDs shape: {data['input_ids'].shape}\")\n",
    "    #print(f\"Attention Mask shape: {data['attention_mask'].shape}\")\n",
    "    #print(f\"Binary Labels: {data['labels'].shape}\")\n",
    "    break  # we only need to check the shape of one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, dev_data, device,  epochs=1, batch_size=16, learning_rate=5e-5): # (amin) added  epochs=3, batch_size=16, learning_rate=5e-5\n",
    "    \"\"\"\n",
    "    Train the model. You can use any training loop you want. We recommend starting with\n",
    "    AdamW as your optimizer. You can take a look at the SST training loop for reference.\n",
    "    Think about your loss function and the number of epochs you want to train for.\n",
    "    You can also use the evaluate_model function to evaluate the\n",
    "    model on the dev set. Print the training loss, training accuracy, and dev accuracy at\n",
    "    the end of each epoch.\n",
    "\n",
    "    Return the trained model.\n",
    "    \"\"\"\n",
    "    ### TODO\n",
    "\n",
    "    # (amin) [\n",
    "    # raise NotImplementedError\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate )\n",
    "    best_dev_acc = float(\"-inf\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Run for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        loss_fn = nn.BCELoss()\n",
    "\n",
    "        for batch in tqdm(\n",
    "            train_data, desc=f\"train-{epoch+1:02}\", disable=TQDM_DISABLE\n",
    "        ):\n",
    "            #b_ids, b_mask, b_labels = (\n",
    "            #    batch[\"input_ids\"],\n",
    "            #    batch[\"attention_mask\"],\n",
    "            #    batch[\"labels\"],\n",
    "            #)\n",
    "            b_ids, b_mask, b_labels = batch\n",
    "\n",
    "            b_ids = b_ids.to(device)\n",
    "            b_mask = b_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() # what is the difference with model.zero_grad() # check later\n",
    "            logits = model(input_ids=b_ids, attention_mask=b_mask)\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        train_loss = train_loss / num_batches\n",
    "\n",
    "        train_acc = evaluate_model(model, train_data, device)\n",
    "        dev_acc =  evaluate_model(model, dev_data, device)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:02}: train loss :: {train_loss:.3f}, train :: {train_acc:.3f}, dev :: {dev_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "        if dev_acc > best_dev_acc:\n",
    "            best_dev_acc = dev_acc\n",
    "            #save_model(model, optimizer, args, config, args.filepath)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_model(model, train_data, dev_data, device)\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate_model(model, dev_data, device)\n",
    "print(f\"The accuracy of the model is: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_data, test_ids, device):\n",
    "    \"\"\"\n",
    "    Test the model. Predict the paraphrase types for the given sentences and return the results in form of\n",
    "    a Pandas dataframe with the columns 'id' and 'Predicted_Paraphrase_Types'.\n",
    "    The 'Predicted_Paraphrase_Types' column should contain the binary array of your model predictions.\n",
    "    Return this dataframe.\n",
    "\n",
    "    Args:\n",
    "        model: The BART model to test.\n",
    "        test_data: The test dataset.\n",
    "        test_ids: The IDs corresponding to the test dataset.\n",
    "        device: The device to use for testing ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        A Pandas DataFrame with 'id' and 'Predicted_Paraphrase_Types' columns.\n",
    "    \"\"\"\n",
    "    ### TODO\n",
    "\n",
    "    # (amin) [\n",
    "    #raise NotImplementedError\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    predictions_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data):\n",
    "            #b_input_ids = batch['input_ids'].to(device)\n",
    "            #b_attention_mask = batch['attention_mask'].to(device)\n",
    "            #b_ids = batch['ids']\n",
    "            b_input_ids, b_attention_mask = batch # _ is for safety\n",
    "            \n",
    "            outputs = model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "            \n",
    "            preds = (outputs > 0.5).int().cpu().numpy()\n",
    "\n",
    "            predictions_list.extend(preds)\n",
    "\n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'Predicted_Paraphrase_Types': predictions_list\n",
    "    })\n",
    "\n",
    "    return results_df\n",
    "\n",
    "    # (amin) ]\n",
    "\n",
    "\n",
    "test_ids = test_dataset[\"id\"]\n",
    "test_results = test_model(model, test_data, test_ids, device)\n",
    "test_results.to_csv(\n",
    "    \"../predictions/bart/etpc-paraphrase-detection-test-output.csv\", index=False, sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playground\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "# Example sentences\n",
    "sentence1 = [\"The cat is on the mat.\", 'Amin is a good boy']\n",
    "sentence2 = [\"There is a cat on the mat.\", 'Amin is cool']\n",
    "encoded = tokenizer(\n",
    "    sentence1,\n",
    "    sentence2,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=50,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "print(encoded)\n",
    "\n",
    "# Decode input_ids back to text\n",
    "decoded_sentence1 = tokenizer.decode(encoded['input_ids'][0], skip_special_tokens=True)\n",
    "decoded_sentence2 = tokenizer.decode(encoded['input_ids'][1], skip_special_tokens=True)\n",
    "\n",
    "print(\"Decoded sentence 1:\", decoded_sentence1)\n",
    "print(\"Decoded sentence 2:\", decoded_sentence2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dnlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
